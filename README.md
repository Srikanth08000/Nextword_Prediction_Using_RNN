# ğŸ”® Next Word Prediction Using LSTM

## ğŸ“– Project Overview
This project develops an **AI-powered Next Word Prediction model** using **Long Short-Term Memory (LSTM)** networks. The model is trained on Shakespeare's **Hamlet** and predicts the next word based on a given sequence. The application is deployed using **Streamlit**, allowing users to interact with the model in real-time.

---

## ğŸ¯ Objectives
âœ”ï¸ Train an **LSTM model** for next-word prediction.  
âœ”ï¸ Implement **text preprocessing** (tokenization & padding).  
âœ”ï¸ Utilize **early stopping** to prevent overfitting.  
âœ”ï¸ Save the trained model (`.h5` file) and tokenizer (`.pickle` file).  
âœ”ï¸ Deploy an **interactive Streamlit web app** for real-time predictions.  

---

## ğŸ› ï¸ Technologies Used
- **Python** â€“ Programming language  
- **TensorFlow/Keras** â€“ Deep learning framework  
- **Streamlit** â€“ Web application framework  
- **NumPy** â€“ Numerical computing  
- **Pickle** â€“ Model and tokenizer serialization  
- **Natural Language Processing (NLP)** â€“ Text processing  

---

## ğŸ“‚ Dataset Used
The dataset used is **Shakespeare's "Hamlet"**, which provides **rich and complex text** for training the model. The text data is **tokenized, sequenced, and padded** to maintain uniform input sizes.

---

## ğŸ” Project Breakdown

### **1ï¸âƒ£ Data Preprocessing**
- **Tokenization** â€“ Convert text into numerical sequences.  
- **Padding** â€“ Ensures all sequences are of the same length.  
- **Splitting** â€“ Training and testing sets are created.  

### **2ï¸âƒ£ LSTM Model Architecture**
The model consists of:
- **Embedding Layer** â€“ Converts words into dense vectors.
- **Two LSTM Layers** â€“ Handles long-term dependencies.
- **Dense Output Layer (Softmax Activation)** â€“ Predicts the next word.

### **3ï¸âƒ£ Model Training**
- **Early Stopping** prevents overfitting by monitoring validation loss.  
- Model is trained using **Categorical Cross-Entropy Loss** and **Adam Optimizer**.

### **4ï¸âƒ£ Model Evaluation**
- Tested on sample text sequences.
- Predicts the next word **with high accuracy**.

### **5ï¸âƒ£ Deployment with Streamlit**
- Users enter a **sequence of words**, and the model predicts the **next word**.
- The UI is **simple, interactive, and real-time**.

---

## ğŸ› ï¸ Installation Guide

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/your-username/next-word-prediction.git
cd next-word-prediction
2ï¸âƒ£ Install Dependencies
bash
Copy
Edit
pip install -r requirements.txt
3ï¸âƒ£ Ensure Model & Tokenizer Files Are Available
Make sure the following files are present in the project directory:

next_word_lstm.h5 â†’ Pre-trained LSTM model
tokenizer.pickle â†’ Saved tokenizer file
If not, train the model again before proceeding.

ğŸš€ How to Run the Streamlit App
bash
Copy
Edit
streamlit run app.py
This will launch a web interface where users can input a phrase and receive AI-generated next-word predictions.

ğŸ“¦ Files & Directories
File/Directory	Description
app.py	Streamlit web app script
next_word_lstm.h5	Trained LSTM model
tokenizer.pickle	Saved tokenizer for word sequences
requirements.txt	Required Python dependencies
ğŸ“š Example Usage
User Input:
plaintext
Copy
Edit
To be or not to
Model Prediction:
plaintext
Copy
Edit
Next word: "be"
ğŸ§© Customization & Improvements
ğŸ”¹ Train on a larger dataset to improve predictions.
ğŸ”¹ Implement Bidirectional LSTM for better text generation.
ğŸ”¹ Use attention mechanisms to enhance accuracy.
ğŸ”¹ Fine-tune hyperparameters for optimized performance.

ğŸ›¡ï¸ Best Practices & Notes
âœ”ï¸ Ensure TensorFlow/Keras is installed for model inference.
âœ”ï¸ Avoid excessive trainingâ€”use early stopping to prevent overfitting.
âœ”ï¸ Make sure .h5 and .pickle files exist before running the app.
âœ”ï¸ Optimize tokenization & preprocessing for better predictions.

